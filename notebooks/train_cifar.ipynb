{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using ClassifierTrainer to train CIFAR dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "We import the ClassifierTrainer class, and also the learning rate scheduler functions from PyTorchTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyTorchTrainer import (\n",
    "    ClassifierTrainer,\n",
    "    get_cosine_lr_scheduler,\n",
    "    get_multiplicative_lr_scheduler\n",
    ")\n",
    "import os, sys, getopt, pickle\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "Below we define a CNN architecture.\n",
    "\n",
    "Note here that we also implement the `get_parameters()` method that returns two list: the first list contains parameters (such as BN paramers) that we dont want to apply weight decay regularization during training, and the second list contains parameters that we would like to apply weight decay regularization.\n",
    "\n",
    "Note that if `get_parameters()` is not implemented and if weight decay coefficient is not 0, then weight decay will be applied to all parameters during training when using the ClassifierTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllCNN(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super(AllCNN, self).__init__()\n",
    "\n",
    "        self.block1 = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=3, out_channels=36, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(36),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=36, out_channels=96, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=96, out_channels=96, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        ])\n",
    "\n",
    "        self.block2 = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=96, out_channels=192, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        ])\n",
    "\n",
    "        self.block3 = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "\n",
    "        self.classifier = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=192, out_channels=n_class, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(n_class),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.block1:\n",
    "            x = layer(x)\n",
    "        for layer in self.block2:\n",
    "            x = layer(x)\n",
    "        for layer in self.block3:\n",
    "            x = layer(x)\n",
    "        for layer in self.classifier:\n",
    "            x = layer(x)\n",
    "        x = x.mean(dim=-1).mean(dim=-1)\n",
    "        return x\n",
    "\n",
    "    def initialize(self,):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "                nn.init.constant_(layer.bias, 0.0)\n",
    "            elif isinstance(layer, nn.BatchNorm2d):\n",
    "                nn.init.constant_(layer.weight, 1)\n",
    "                nn.init.constant_(layer.bias, 0.0)\n",
    "            elif isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "                nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def get_parameters(self,):\n",
    "        bn_params = list(self.block1[1].parameters()) +\\\n",
    "            list(self.block1[4].parameters()) +\\\n",
    "            list(self.block1[7].parameters()) +\\\n",
    "            list(self.block2[1].parameters()) +\\\n",
    "            list(self.block2[4].parameters()) +\\\n",
    "            list(self.block2[7].parameters()) +\\\n",
    "            list(self.block3[1].parameters()) +\\\n",
    "            list(self.block3[4].parameters()) +\\\n",
    "            list(self.classifier[1].parameters())\n",
    "\n",
    "        other_params = list(self.block1[0].parameters()) +\\\n",
    "            list(self.block1[3].parameters()) +\\\n",
    "            list(self.block1[6].parameters()) +\\\n",
    "            list(self.block2[0].parameters()) +\\\n",
    "            list(self.block2[3].parameters()) +\\\n",
    "            list(self.block2[6].parameters()) +\\\n",
    "            list(self.block3[0].parameters()) +\\\n",
    "            list(self.block3[3].parameters()) +\\\n",
    "            list(self.classifier[0].parameters())\n",
    "\n",
    "        return bn_params, other_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loader objects\n",
    "\n",
    "Below we implement the function that returns the dataloaders for CIFAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset):\n",
    "    mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "    std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "    train_transform = transforms.Compose(\n",
    "        [transforms.RandomHorizontalFlip(),\n",
    "         transforms.RandomCrop(32, padding=4),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean, std)])\n",
    "\n",
    "    test_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize(mean, std)])\n",
    "\n",
    "    if dataset == 'cifar10':\n",
    "        trainset = torchvision.datasets.CIFAR10(root='.', train=True,\n",
    "                                                download=True, transform=train_transform)\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root='.', train=False,\n",
    "                                               download=True, transform=test_transform)\n",
    "\n",
    "        n_class = 10\n",
    "    else:\n",
    "        trainset = torchvision.datasets.CIFAR100(root='.', train=True,\n",
    "                                                download=True, transform=train_transform)\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR100(root='.', train=False,\n",
    "                                               download=True, transform=test_transform)\n",
    "        n_class = 100\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=64, pin_memory=True, shuffle=True)\n",
    "    test_loader = DataLoader(testset, batch_size=64, pin_memory=True, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, n_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code\n",
    "\n",
    "For the main code, we will:\n",
    "- get dataloaders\n",
    "- create a model instance\n",
    "- create a trainer and train the model using the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# here we load the cifar10 dataset\n",
    "train_loader, test_loader, n_class = get_dataset('cifar10')\n",
    "\n",
    "# then create a model instance\n",
    "model = AllCNN(n_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer options\n",
    "\n",
    "The `ClassifierTrainer` class comes with many options. The only required argument is `n_epoch`, which specifies the number of epochs to train the model. By default, `ClassifierTrainer` uses a learning rate scheduler that gradually reduces the learning rate using cosine function. The default starting learning rate is `1e-3` and the last learning rate is `1e-5`. To specify another starting and ending learning rate for the cosine learning rate scheduler, we can use the convenient function `get_cosine_lr_scheduler(initial_lr, final_lr)`.\n",
    "\n",
    "For example, we will create a trainer object with cosine learning rate scheduler starting from 0.01 down to 0.0001 for 200 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ClassifierTrainer(n_epoch=200,\n",
    "                            lr_scheduler=get_cosine_lr_scheduler(1e-2, 1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can specify a multiplicative learning rate with an initial learning rate, the epoch indices when the learning rate drops, and the multiplication factor using the convenient function `get_multiplicative_lr_scheduler(init_lr, drop_at, multiplicative_factor)`\n",
    "\n",
    "For example, below we will specify a learning rate scheduler that starts with a learning rate of `1e-2` and drop by a factor of `0.1` at epoch 30, 60, 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ClassifierTrainer(n_epoch=200,\n",
    "                            lr_scheduler=get_multiplicative_lr_scheduler(1e-2, [30, 60, 90], 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ClassifierTrainer` class has other options such as saving checkpoints (given that `temp_dir` is specified) at a specified frequency `checkpoint_freq`, training from a particular checkpoint `epoch_idx` if exists. Please consult the complete description of the interface to know more. \n",
    "\n",
    "After the trainer is constructed, we can train the model using `fit()`. This function will return a dictionary that contains keys:\n",
    "\n",
    "`train_acc`, `train_cross_entropy`, \n",
    "\n",
    "`val_acc`, `val_cross_entropy` if `val_loader` is not None\n",
    "\n",
    "`test_acc`, `test_cross_entropy` if `test_loader` is not None\n",
    "\n",
    "When accessing each key, we will get the list of performance measured at each epoch. For example `performance['train_acc']` is the training accuracy curve during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = trainer.fit(model=model,\n",
    "                          train_loader=train_loader,\n",
    "                          val_loader=None,\n",
    "                          test_loader=test_loader,\n",
    "                          device=torch.device('cuda'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
